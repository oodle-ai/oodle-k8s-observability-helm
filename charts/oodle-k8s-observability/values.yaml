# Configuration for Oodle - ConfigMap for centralized configuration
# 
# IMPORTANT: Before installing this chart, create a Kubernetes Secret with your Oodle API key:
#   kubectl create secret generic oodle-api-key --from-literal=apiKey=YOUR_API_KEY
# 
# The chart expects a secret named 'oodle-api-key' with key 'apiKey' by default.
# If using a different secret name, override the secretKeyRef in vmagent, vector-aggregator, and event-exporter.
oodleConfig:
  # Enable ConfigMap creation by this chart
  # Set to false if you want to manage the ConfigMap externally
  # If false, you must create a ConfigMap named 'oodle-k8s-observability-config' with these keys:
  #   - clusterName
  #   - oodleInstance
  #   - oodleLogsHost
  #   - oodleMetricsHost
  enabled: true
  
  # Cluster name for Kubernetes cluster identification
  clusterName: "" # Kubernetes Cluster Name
  
  # Oodle instance identifier
  oodleInstance: "" # Oodle Instance (e.g., inst-your-instance-id)
  
  # Oodle logs host endpoint (optional)
  # If not specified, defaults to: https://{oodleInstance}-logs.collector.oodle.ai
  oodleLogsHost: "" # Oodle Logs Host (e.g., https://inst-your-instance-logs.collector.oodle.ai)
  
  # Oodle metrics host endpoint (optional)
  # If not specified, defaults to: https://{oodleInstance}.collector.oodle.ai
  oodleMetricsHost: "" # Oodle Metrics Host (e.g., https://inst-your-instance.collector.oodle.ai)

# Configuration for kube-state-metrics chart
kube-state-metrics:
  enabled: true
  
  # Override labels to make this instance uniquely identifiable
  customLabels:
    app: oodle-kube-state-metrics

# Configuration for prometheus-node-exporter chart  
prometheus-node-exporter:
  enabled: true
  
  # Use custom port to avoid conflicts with other node exporters
  service:
    port: 9301
    targetPort: 9301
    
  # Override labels to make this instance uniquely identifiable
  commonLabels:
    app: oodle-node-exporter

# Configuration for Metrics
vmagent:
  enabled: true

  mode: statefulSet

  # Number of vmagent replicas for sharding
  replicaCount: 2

  # StatefulSet configuration for sharding
  statefulSet:
    clusterMode: true
    replicationFactor: 1
    spec:
      podManagementPolicy: "Parallel"

  # Storage configuration for scrape queue
  # The master switch is: persistentVolume.enabled
  # - If enabled: true  → Uses PVC (emptyDir config is IGNORED)
  # - If enabled: false → Uses emptyDir (persistentVolume.size is IGNORED)
  # 
  # Option 1: Persistent Volume (RECOMMENDED for production)
  # - Survives pod restarts and node failures
  # - Queued metrics are preserved during outages
  # - Required for reliable metric delivery
  # 
  # Option 2: EmptyDir (for dev/testing or low-criticality environments)
  # - Faster performance (especially with memory-backed tmpfs)
  # - Data lost on pod restart
  # - Lower resource requirements (no PVC needed)

  # Persistent volume for each replica to store scrape queue
  persistentVolume:
    enabled: false  # ← MASTER SWITCH: Set to true to use persistentVolume instead
    size: 20Gi
    storageClass: ""
    # Maximum disk usage per remote write URL (set in extraArgs below)
    # Recommended: 85% of PVC size to leave headroom for filesystem overhead
    # Current: 20Gi * 0.85 = 17GB
    # If you change 'size' above, update remoteWrite.maxDiskUsagePerURL in extraArgs proportionally

  # EmptyDir configuration (ONLY used when persistentVolume.enabled: false)
  emptyDir:
    sizeLimit: 20Gi
  #  
  # Note: Always update remoteWrite.maxDiskUsagePerURL in extraArgs
  #       to ~85% of sizeLimit (e.g., 20Gi → 17GB)

  # Resource limits for each replica
  resources:
    limits:
      cpu: 1000m
      memory: 4Gi
    requests:
      cpu: 100m
      memory: 512Mi

  # Pod disruption budget to ensure availability during updates
  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  # Pod anti-affinity to spread replicas across different nodes
  # Using "preferred" (soft requirement) instead of "required" (hard requirement)
  # to allow deployment on single-node clusters while still preferring node spreading
  # 
  # Behavior:
  # - Multi-node cluster: Pods will be spread across different nodes (weight: 100)
  # - Single-node cluster: Both pods can run on the same node (better than Pending)
  # 
  # To enforce HARD requirement (pods won't schedule on single-node):
  # Change "preferredDuringScheduling..." to "requiredDuringScheduling..."
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmagent
                app.kubernetes.io/instance: oodle-observability  # ← UPDATE THIS to your release name
            topologyKey: kubernetes.io/hostname

  # Pod labels (optional - for additional custom labels beyond the defaults)
  # The vmagent chart already sets standard Kubernetes labels automatically:
  # - app.kubernetes.io/name, app.kubernetes.io/instance, etc.
  # Add custom labels here if needed for your specific use cases
  podLabels: {}

  # Add prometheus annotations for self-discovery
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8429"
    prometheus.io/path: "/metrics"

  # Environment variables from ConfigMap and Secret
  env:
    - name: OODLE_API_KEY
      valueFrom:
        secretKeyRef:
          name: oodle-api-key
          key: apiKey
    - name: CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: clusterName
    - name: OODLE_INSTANCE
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: oodleInstance
    - name: OODLE_METRICS_HOST
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: oodleMetricsHost

  # Remote write configuration for sending metrics to Oodle
  remoteWrite:
    - url: "%{OODLE_METRICS_HOST}/v1/prometheus/%{OODLE_INSTANCE}/write"
      headers: "X-API-KEY: %{OODLE_API_KEY}"
      forcePromProto: true

  extraArgs:
    promscrape.suppressDuplicateScrapeTargetErrors: "true"
    # Maximum disk space per remote write URL for persistent queue
    # ⚠️  MUST be set to ~85% of your storage size (PersistentVolume or emptyDir)
    # Currently set for: 20Gi storage → 20Gi * 0.85 = 17GB
    # Update this value whenever persistentVolume.size or emptyDir.sizeLimit changes!
    #
    # Quick Reference (85% of storage):
    #   10Gi → 8.5GB  |  20Gi → 17GB  |  50Gi → 42GB  |  100Gi → 85GB
    remoteWrite.maxDiskUsagePerURL: "17GB"

  # -- Extra scrape configs that will be appended to `config`
  extraScrapeConfigs: [ ]

  # =============================================================================
  # MANAGED SCRAPE CONFIG (OPT-IN FEATURE)
  # =============================================================================
  # When enabled, this chart manages the vmagent scrape configuration via a
  # dedicated ConfigMap, allowing you to enable/disable individual scrape jobs
  # and add metric drop rules per-job or globally.
  # 
  # ⚠️  IMPORTANT: This is disabled by default for backwards compatibility.
  # Existing users who have customized vmagent.config.scrape_configs should
  # continue to use that approach unless they want to migrate to this new system.
  # 
  # To enable managed scrape config, add BOTH settings to your values:
  # 
  #   vmagent:
  #     managedScrapeConfig:
  #       enabled: true
  #     configMap: "vmagent-scrape-config"  # Required when managedScrapeConfig is enabled
  # 
  # When enabled:
  # - A ConfigMap 'vmagent-scrape-config' is created with conditional scrape jobs
  # - The 'scrapeJobs' section below controls which jobs are enabled
  # - The 'globalMetricRelabelConfigs' applies drop rules to all jobs
  # - The 'vmagent.config' section is IGNORED (use scrapeJobs instead)
  # 
  # When disabled (default):
  # - The 'vmagent.config' section below is used as-is
  # - 'scrapeJobs' and 'globalMetricRelabelConfigs' have no effect
  # =============================================================================
  managedScrapeConfig:
    enabled: false  # Set to true to use configurable scrapeJobs
    # Global scrape settings (only used when managedScrapeConfig.enabled: true)
    scrape_interval: 60s
    scrape_timeout: 1m
  
  # ⚠️  NOTE:
  # If managedScrapeConfig.enabled is true, then configMap must be set to
  # "vmagent-scrape-config" to use the managed scrape config.
  # configMap: "vmagent-scrape-config"

  # -----------------------------------------------------------------------------
  # SCRAPE JOBS CONFIGURATION (only used when managedScrapeConfig.enabled: true)
  # -----------------------------------------------------------------------------
  # Enable/disable individual scrape jobs as needed.
  # Each job can also have custom metric_relabel_configs to drop/modify metrics.
  # 
  # To disable a scrape job, set its 'enabled' field to false.
  # To add metric drop rules, add entries to 'metric_relabel_configs' for that job.
  # -----------------------------------------------------------------------------
  scrapeJobs:
    # Scrape Kubernetes API servers
    kubernetesApiServers:
      enabled: true
      # Per-job overrides (optional - defaults to global managedScrapeConfig values):
      # scrape_interval: 60s
      # scrape_timeout: 1m
      # Add custom metric_relabel_configs to drop/modify metrics
      # Example to drop metrics by prefix:
      # metric_relabel_configs:
      #   - source_labels: [__name__]
      #     regex: "apiserver_request_duration_seconds_bucket"
      #     action: drop

    # Scrape Kubelet metrics from each node
    kubernetesNodes:
      enabled: true
      # scrape_interval: 60s
      # scrape_timeout: 1m
      # metric_relabel_configs: []

    # Scrape cAdvisor metrics (container metrics)
    kubernetesNodesCadvisor:
      enabled: true
      # scrape_interval: 60s
      # scrape_timeout: 1m
      # metric_relabel_configs: []

    # Scrape kube-state-metrics
    kubeStateMetrics:
      enabled: true
      # scrape_interval: 60s
      # scrape_timeout: 1m
      # metric_relabel_configs: []

    # Scrape node-exporter
    nodeExporter:
      enabled: true
      # scrape_interval: 60s
      # scrape_timeout: 1m
      # metric_relabel_configs: []

    # Scrape pods with prometheus.io/scrape: true annotation
    kubernetesPods:
      enabled: true
      # scrape_interval: 60s
      # scrape_timeout: 1m
      # metric_relabel_configs: []

    # Scrape Beyla eBPF metrics
    oodleBeyla:
      enabled: true
      # scrape_interval: 60s
      # scrape_timeout: 1m
      # metric_relabel_configs: []

  # =============================================================================
  # SCRAPE CONFIGURATION (DEFAULT - used when managedScrapeConfig.enabled: false)
  # =============================================================================
  # This is the default scrape configuration used by vmagent.
  # Customize this section directly if you need to modify scrape jobs without
  # using the managedScrapeConfig feature.
  # 
  # ⚠️  NOTE: This section is IGNORED when managedScrapeConfig.enabled: true
  # =============================================================================
  config:
    global:
      scrape_interval: 60s
      scrape_timeout: 1m
      external_labels:
        cluster: "%{CLUSTER_NAME}"

    scrape_configs:
      # Note: vmagent scrapes itself via prometheus.io/scrape annotation
      # discovered by kubernetes-pods job below

      # Scrape Kubernetes API servers
      - job_name: kubernetes-apiservers
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # Scrape Kubelet metrics from each node using direct mode
      - job_name: kubernetes-nodes
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        metrics_path: /metrics
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - source_labels: [__address__]
            regex: ([^:]+):(.*)
            target_label: __address__
            replacement: $1:10250
          # Set instance to node name (hostname) instead of IP
          - source_labels: [__meta_kubernetes_node_name]
            target_label: instance
            replacement: $1
          - source_labels: [ __meta_kubernetes_node_name ]
            target_label: node
            replacement: $1
        honor_timestamps: false

      # Scrape cAdvisor metrics (container metrics) using direct mode
      - job_name: kubernetes-nodes-cadvisor
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        metrics_path: /metrics/cadvisor
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - source_labels: [__address__]
            regex: ([^:]+):(.*)
            target_label: __address__
            replacement: $1:10250
          # Set instance to node name (hostname) instead of IP
          - source_labels: [__meta_kubernetes_node_name]
            target_label: instance
            replacement: $1
        honor_timestamps: false

      # Scrape kube-state-metrics (only the one deployed by this chart)
      - job_name: kube-state-metrics
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          # Filter by custom label to ensure we only scrape our deployed instance
          - source_labels: [__meta_kubernetes_service_label_app]
            action: keep
            regex: "oodle-kube-state-metrics"
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: ".*kube-state-metrics.*"
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http

      # Scrape node-exporter (only the one deployed by this chart)
      - job_name: node-exporter
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          # Filter by custom label to ensure we only scrape our deployed instance
          - source_labels: [__meta_kubernetes_service_label_app]
            action: keep
            regex: "oodle-node-exporter"
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: ".*node-exporter.*"
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: instance
            replacement: $1

      # Scrape pods with prometheus.io/scrape: true annotation
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: drop
            source_labels: [__meta_kubernetes_pod_container_init]
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node
          - source_labels: [ __meta_kubernetes_pod_node_name ]
            target_label: instance
            replacement: $1

      # Scrape beyla with hardcoded oodle_metrics path
      - job_name: oodle-beyla
        kubernetes_sd_configs:
          - role: pod
        metrics_path: /oodle_metrics
        relabel_configs:
          - action: drop
            source_labels: [__meta_kubernetes_pod_container_init]
            regex: true
          # Only scrape beyla pods
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: keep
            regex: ".*beyla.*"
          # Use prometheus port annotation if present, otherwise default
          - source_labels:
              [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node
          - source_labels: [ __meta_kubernetes_pod_node_name ]
            target_label: instance
            replacement: $1
          # Set cardinality limit for beyla oodle metrics
          - target_label: __series_limit__
            replacement: "1000000"

# Configuration for oodle-k8s-auto-instrumentation chart (using alias: auto-instrumentation)
auto-instrumentation:
  enabled: true
  beyla:
    # Use envValueFrom to reference ConfigMap values
    envValueFrom:
      BEYLA_KUBE_CLUSTER_NAME:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: clusterName

# Configuration for Vector Agent
vector-agent:
  enabled: true
  role: Agent

  env:
    - name: CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: clusterName

  tolerations:
    - effect: NoSchedule
      operator: Exists

  # Additional filters applied to select which pods to scrape
  # Example: "app.kubernetes.io/name!=kube-state-metrics,app.kubernetes.io/name!=node-exporter"
  label_selectors: ""
  # Additional filters applied to select which namespaces to scrape
  # Example: "kubernetes.io/metadata.name!=kube-system,kubernetes.io/metadata.name!=kube-public"
  namespace_label_selectors: ""

  customConfig:
    data_dir: /vector-data-dir
    api:
      enabled: true
      address: 127.0.0.1:8686
      playground: false

    sources:
      kubernetes_logs:
        type: kubernetes_logs
        max_line_bytes: 1048576 # 1 Mib
        extra_label_selector: "{{ .Values.label_selectors }}"
        extra_namespace_label_selector: "{{ .Values.namespace_label_selectors }}"

    transforms:
      drop_old_logs:
        type: filter
        inputs:
          - kubernetes_logs
        condition: |
          to_unix_timestamp(now()) - 24*3600 < to_unix_timestamp(timestamp!(.timestamp))

      add_cluster_name:
        type: remap
        inputs:
          - drop_old_logs
        source: |
          if !exists(.cluster) {
            .cluster = "${CLUSTER_NAME}"
          }
          .

    sinks:
      vector:
        type: vector
        inputs:
          - add_cluster_name
        address: "{{ .Release.Name }}-vector-aggregator:6000"

# Configuration for Vector Aggregator  
vector-aggregator:
  enabled: true
  env:
    - name: OODLE_INSTANCE
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: oodleInstance
    - name: OODLE_API_KEY
      valueFrom:
        secretKeyRef:
          name: oodle-api-key
          key: apiKey
    - name: OODLE_LOGS_HOST
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: oodleLogsHost

  podLabels:
    # Make sure `vector-agent` collects logs from `vector-aggregator`.
    "vector.dev/exclude": "false"

  customConfig:
    data_dir: /vector-data-dir
    api:
      enabled: true
      address: 127.0.0.1:8686
      playground: false

    sources:
      vector:
        address: 0.0.0.0:6000
        type: vector
    
    sinks:
      oodle:
        type: http
        inputs:
          - vector
        uri: "${OODLE_LOGS_HOST}/ingest/v1/logs"
        encoding:
          codec: json
        compression: gzip
        request:
          headers:
            X-OODLE-INSTANCE: "${OODLE_INSTANCE}"
            X-API-KEY: "${OODLE_API_KEY}"
            X-OODLE-LOG-SOURCE: "vector"
          retry_attempts: 3
          timeout_secs: 60

# Configuration for Kubernetes Event Exporter
event-exporter:
  enabled: true

  image:
    registry: "public.ecr.aws/oodle-ai"
    repository: kubernetes-event-exporter
    tag: 1.7.0-debian-12-r46

  global:
    security:
      allowInsecureImages: true

  extraEnvVars:
    - name: CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: clusterName
    - name: OODLE_INSTANCE
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: oodleInstance
    - name: OODLE_API_KEY
      valueFrom:
        secretKeyRef:
          name: oodle-api-key
          key: apiKey
    - name: OODLE_LOGS_HOST
      valueFrom:
        configMapKeyRef:
          name: oodle-k8s-observability-config
          key: oodleLogsHost

  config:
    clusterName: "${CLUSTER_NAME}"
    maxEventAgeSeconds: 600
    logLevel: info
    logFormat: json
    route:
      routes:
        - match:
            - receiver: "oodle"

    receivers:
      - name: "oodle"
        webhook:
          endpoint: "${OODLE_LOGS_HOST}/ingest/v1/logs"
          headers:
            X-OODLE-INSTANCE: "${OODLE_INSTANCE}"
            X-API-KEY: "${OODLE_API_KEY}"
            X-OODLE-LOG-SOURCE: "event-exporter"
